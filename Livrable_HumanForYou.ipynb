{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd60d035-8c44-424d-9167-0ab80f98842f",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bef16-a45c-45b2-8ef8-27d8d1565cb1",
   "metadata": {},
   "source": [
    "Nous sommes engagés dans un projet visant à examiner les facteurs qui influencent le taux de rotation des employés chez HumanForYou, une entreprise pharmaceutique indienne. Avec un taux de rotation annuel d'environ 15 %, nous cherchons à identifier les causes sous-jacentes et à élaborer des stratégies pour le diminuer. Ce taux élevé de rotation présente plusieurs désavantages pour l'entreprise, y compris des retards dans les projets, la nécessité d'un service des ressources humaines important pour le recrutement, et le temps consacré à la formation des nouveaux employés jusqu'à ce qu'ils atteignent leur pleine productivité.\n",
    "\n",
    "Pour atteindre cet objectif, l'entreprise nous a fourni plusieurs ensembles de données anonymisées contenant des informations sur les employés, leurs évaluations par les managers, leur satisfaction au travail, et leurs horaires de travail pour l'année 2015. Ces données englobent une variété d'aspects tels que l'âge, le sexe, le niveau d'éducation, le domaine d'étude, le salaire mensuel, le nombre d'années passées dans l'entreprise, ainsi que la satisfaction vis-à-vis de l'environnement de travail, du poste occupé, et de l'équilibre travail-vie personnelle.\n",
    "\n",
    "Notre mission en tant qu'analystes de données comprend plusieurs étapes clés :\n",
    "\n",
    "1. Générer et justifier le traitement des jeux de données pour aborder la problématique.\n",
    "2. Sélectionner et justifier les algorithmes d'intelligence artificielle à utiliser.\n",
    "3. Analyser et interpréter les résultats obtenus à l'aide de ces algorithmes, en s'appuyant sur des métriques adéquates.\n",
    "4. Proposer une démarche pour améliorer les modèles d'IA développés.\n",
    "5. Déterminer le modèle final en fonction des besoins spécifiques de l'entreprise et des analyses effectuées.\n",
    "6. Émettre des recommandations basées sur l'analyse pour aider l'entreprise à réduire son taux de rotation.\n",
    "\n",
    "Nous sommes attendus pour présenter nos résultats et recommandations à travers une présentation intégrant un notebook Jupyter, démontrant l'ensemble de notre démarche analytique et les conclusions que nous avons tirées.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32fce97-aecf-4ed0-a259-85abed3f7d29",
   "metadata": {},
   "source": [
    "# Préparation des données\n",
    "## Préparation de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "746ec674-458e-4c20-bae4-89ea97781157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osman\\AppData\\Local\\Temp\\ipykernel_9124\\1442708232.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Importations standard et pour la manipulation des données\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from cmath import phase, rect\n",
    "from math import radians, degrees\n",
    "\n",
    "# Importations pour la visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Importations pour le machine learning\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Configuration de Seaborn pour les visualisations\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Ignorer les avertissements inutiles pour nettoyer la sortie\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3664f74-7d28-4ffa-84d0-02f779b08afa",
   "metadata": {},
   "source": [
    "## Import des données\n",
    "Cinq ensembles de données sont chargés en utilisant la fonction pd.read_csv() de la bibliothèque pandas, une bibliothèque puissante pour la manipulation et l'analyse des données en Python. Chaque fichier CSV est lu et stocké dans un DataFrame pandas distinct, ce qui permet une manipulation aisée des données pour l'analyse ultérieure. Les DataFrames créés sont :\n",
    "\n",
    "- general contenant les données générales sur les employés (fichier general_data.csv).\n",
    "- employe contenant les résultats de l'enquête sur les employés (fichier employee_survey_data.csv).\n",
    "- manager contenant les résultats de l'enquête sur les managers (fichier manager_survey_data.csv).\n",
    "- In contenant les données sur les horaires d'arrivée des employés (fichier in_time.csv).\n",
    "- Out contenant les données sur les horaires de départ des employés (fichier out_time.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ffb6487-97df-4c36-9d50-0a0d652c8171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>2015-01-01</th>\n",
       "      <th>2015-01-02</th>\n",
       "      <th>2015-01-05</th>\n",
       "      <th>2015-01-06</th>\n",
       "      <th>2015-01-07</th>\n",
       "      <th>2015-01-08</th>\n",
       "      <th>2015-01-09</th>\n",
       "      <th>2015-01-12</th>\n",
       "      <th>2015-01-13</th>\n",
       "      <th>...</th>\n",
       "      <th>2015-12-18</th>\n",
       "      <th>2015-12-21</th>\n",
       "      <th>2015-12-22</th>\n",
       "      <th>2015-12-23</th>\n",
       "      <th>2015-12-24</th>\n",
       "      <th>2015-12-25</th>\n",
       "      <th>2015-12-28</th>\n",
       "      <th>2015-12-29</th>\n",
       "      <th>2015-12-30</th>\n",
       "      <th>2015-12-31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-02 09:43:45</td>\n",
       "      <td>2015-01-05 10:08:48</td>\n",
       "      <td>2015-01-06 09:54:26</td>\n",
       "      <td>2015-01-07 09:34:31</td>\n",
       "      <td>2015-01-08 09:51:09</td>\n",
       "      <td>2015-01-09 10:09:25</td>\n",
       "      <td>2015-01-12 09:42:53</td>\n",
       "      <td>2015-01-13 10:13:06</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-21 09:55:29</td>\n",
       "      <td>2015-12-22 10:04:06</td>\n",
       "      <td>2015-12-23 10:14:27</td>\n",
       "      <td>2015-12-24 10:11:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-28 10:13:41</td>\n",
       "      <td>2015-12-29 10:03:36</td>\n",
       "      <td>2015-12-30 09:54:12</td>\n",
       "      <td>2015-12-31 10:12:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-02 10:15:44</td>\n",
       "      <td>2015-01-05 10:21:05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-07 09:45:17</td>\n",
       "      <td>2015-01-08 10:09:04</td>\n",
       "      <td>2015-01-09 09:43:26</td>\n",
       "      <td>2015-01-12 10:00:07</td>\n",
       "      <td>2015-01-13 10:43:29</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-12-18 10:37:17</td>\n",
       "      <td>2015-12-21 09:49:02</td>\n",
       "      <td>2015-12-22 10:33:51</td>\n",
       "      <td>2015-12-23 10:12:10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-28 09:31:45</td>\n",
       "      <td>2015-12-29 09:55:49</td>\n",
       "      <td>2015-12-30 10:32:25</td>\n",
       "      <td>2015-12-31 09:27:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-02 10:17:41</td>\n",
       "      <td>2015-01-05 09:50:50</td>\n",
       "      <td>2015-01-06 10:14:13</td>\n",
       "      <td>2015-01-07 09:47:27</td>\n",
       "      <td>2015-01-08 10:03:40</td>\n",
       "      <td>2015-01-09 10:05:49</td>\n",
       "      <td>2015-01-12 10:03:47</td>\n",
       "      <td>2015-01-13 10:21:26</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-12-18 10:15:14</td>\n",
       "      <td>2015-12-21 10:10:28</td>\n",
       "      <td>2015-12-22 09:44:44</td>\n",
       "      <td>2015-12-23 10:15:54</td>\n",
       "      <td>2015-12-24 10:07:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-28 09:42:05</td>\n",
       "      <td>2015-12-29 09:43:36</td>\n",
       "      <td>2015-12-30 09:34:05</td>\n",
       "      <td>2015-12-31 10:28:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-02 10:05:06</td>\n",
       "      <td>2015-01-05 09:56:32</td>\n",
       "      <td>2015-01-06 10:11:07</td>\n",
       "      <td>2015-01-07 09:37:30</td>\n",
       "      <td>2015-01-08 10:02:08</td>\n",
       "      <td>2015-01-09 10:08:12</td>\n",
       "      <td>2015-01-12 10:13:42</td>\n",
       "      <td>2015-01-13 09:53:22</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-12-18 10:17:38</td>\n",
       "      <td>2015-12-21 09:58:21</td>\n",
       "      <td>2015-12-22 10:04:25</td>\n",
       "      <td>2015-12-23 10:11:46</td>\n",
       "      <td>2015-12-24 09:43:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-28 09:52:44</td>\n",
       "      <td>2015-12-29 09:33:16</td>\n",
       "      <td>2015-12-30 10:18:12</td>\n",
       "      <td>2015-12-31 10:01:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-02 10:28:17</td>\n",
       "      <td>2015-01-05 09:49:58</td>\n",
       "      <td>2015-01-06 09:45:28</td>\n",
       "      <td>2015-01-07 09:49:37</td>\n",
       "      <td>2015-01-08 10:19:44</td>\n",
       "      <td>2015-01-09 10:00:50</td>\n",
       "      <td>2015-01-12 10:29:27</td>\n",
       "      <td>2015-01-13 09:59:32</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-12-18 09:58:35</td>\n",
       "      <td>2015-12-21 10:03:41</td>\n",
       "      <td>2015-12-22 10:10:30</td>\n",
       "      <td>2015-12-23 10:13:36</td>\n",
       "      <td>2015-12-24 09:44:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-28 10:05:15</td>\n",
       "      <td>2015-12-29 10:30:53</td>\n",
       "      <td>2015-12-30 09:18:21</td>\n",
       "      <td>2015-12-31 09:41:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4405</th>\n",
       "      <td>4406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-02 09:20:32</td>\n",
       "      <td>2015-01-05 10:17:53</td>\n",
       "      <td>2015-01-06 10:26:51</td>\n",
       "      <td>2015-01-07 10:06:58</td>\n",
       "      <td>2015-01-08 09:45:06</td>\n",
       "      <td>2015-01-09 09:49:24</td>\n",
       "      <td>2015-01-12 09:37:10</td>\n",
       "      <td>2015-01-13 09:25:02</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-12-18 10:01:06</td>\n",
       "      <td>2015-12-21 10:25:25</td>\n",
       "      <td>2015-12-22 10:16:11</td>\n",
       "      <td>2015-12-23 10:04:40</td>\n",
       "      <td>2015-12-24 09:45:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-28 10:15:39</td>\n",
       "      <td>2015-12-29 10:10:09</td>\n",
       "      <td>2015-12-30 09:28:19</td>\n",
       "      <td>2015-12-31 10:00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4406</th>\n",
       "      <td>4407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-02 10:03:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-06 09:44:00</td>\n",
       "      <td>2015-01-07 09:42:10</td>\n",
       "      <td>2015-01-08 10:00:57</td>\n",
       "      <td>2015-01-09 09:44:04</td>\n",
       "      <td>2015-01-12 10:07:32</td>\n",
       "      <td>2015-01-13 10:05:11</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-12-18 09:27:32</td>\n",
       "      <td>2015-12-21 09:41:24</td>\n",
       "      <td>2015-12-22 09:50:30</td>\n",
       "      <td>2015-12-23 10:32:21</td>\n",
       "      <td>2015-12-24 09:47:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-28 09:54:23</td>\n",
       "      <td>2015-12-29 10:13:32</td>\n",
       "      <td>2015-12-30 10:21:09</td>\n",
       "      <td>2015-12-31 10:09:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4407</th>\n",
       "      <td>4408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-02 10:01:01</td>\n",
       "      <td>2015-01-05 09:33:00</td>\n",
       "      <td>2015-01-06 09:49:17</td>\n",
       "      <td>2015-01-07 10:28:12</td>\n",
       "      <td>2015-01-08 09:47:38</td>\n",
       "      <td>2015-01-09 10:01:03</td>\n",
       "      <td>2015-01-12 09:49:12</td>\n",
       "      <td>2015-01-13 09:47:10</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-12-18 10:00:57</td>\n",
       "      <td>2015-12-21 09:51:07</td>\n",
       "      <td>2015-12-22 10:02:10</td>\n",
       "      <td>2015-12-23 09:58:29</td>\n",
       "      <td>2015-12-24 09:56:05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-28 09:59:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-30 10:02:36</td>\n",
       "      <td>2015-12-31 10:03:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4408</th>\n",
       "      <td>4409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-02 10:17:05</td>\n",
       "      <td>2015-01-05 10:02:27</td>\n",
       "      <td>2015-01-06 10:12:50</td>\n",
       "      <td>2015-01-07 10:12:31</td>\n",
       "      <td>2015-01-08 09:42:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-12 10:00:38</td>\n",
       "      <td>2015-01-13 09:48:03</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-12-18 09:54:33</td>\n",
       "      <td>2015-12-21 10:01:08</td>\n",
       "      <td>2015-12-22 10:10:19</td>\n",
       "      <td>2015-12-23 09:42:30</td>\n",
       "      <td>2015-12-24 09:56:05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-28 09:55:25</td>\n",
       "      <td>2015-12-29 09:54:42</td>\n",
       "      <td>2015-12-30 10:15:44</td>\n",
       "      <td>2015-12-31 09:56:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4409</th>\n",
       "      <td>4410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-02 09:59:09</td>\n",
       "      <td>2015-01-05 10:16:14</td>\n",
       "      <td>2015-01-06 09:52:30</td>\n",
       "      <td>2015-01-07 09:43:15</td>\n",
       "      <td>2015-01-08 10:06:55</td>\n",
       "      <td>2015-01-09 10:27:39</td>\n",
       "      <td>2015-01-12 09:47:35</td>\n",
       "      <td>2015-01-13 09:30:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-12-18 10:20:40</td>\n",
       "      <td>2015-12-21 10:26:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-23 09:53:19</td>\n",
       "      <td>2015-12-24 09:42:13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-28 09:41:11</td>\n",
       "      <td>2015-12-29 10:05:15</td>\n",
       "      <td>2015-12-30 10:32:49</td>\n",
       "      <td>2015-12-31 10:09:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4410 rows × 262 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  2015-01-01           2015-01-02           2015-01-05  \\\n",
       "0              1         NaN  2015-01-02 09:43:45  2015-01-05 10:08:48   \n",
       "1              2         NaN  2015-01-02 10:15:44  2015-01-05 10:21:05   \n",
       "2              3         NaN  2015-01-02 10:17:41  2015-01-05 09:50:50   \n",
       "3              4         NaN  2015-01-02 10:05:06  2015-01-05 09:56:32   \n",
       "4              5         NaN  2015-01-02 10:28:17  2015-01-05 09:49:58   \n",
       "...          ...         ...                  ...                  ...   \n",
       "4405        4406         NaN  2015-01-02 09:20:32  2015-01-05 10:17:53   \n",
       "4406        4407         NaN  2015-01-02 10:03:41                  NaN   \n",
       "4407        4408         NaN  2015-01-02 10:01:01  2015-01-05 09:33:00   \n",
       "4408        4409         NaN  2015-01-02 10:17:05  2015-01-05 10:02:27   \n",
       "4409        4410         NaN  2015-01-02 09:59:09  2015-01-05 10:16:14   \n",
       "\n",
       "               2015-01-06           2015-01-07           2015-01-08  \\\n",
       "0     2015-01-06 09:54:26  2015-01-07 09:34:31  2015-01-08 09:51:09   \n",
       "1                     NaN  2015-01-07 09:45:17  2015-01-08 10:09:04   \n",
       "2     2015-01-06 10:14:13  2015-01-07 09:47:27  2015-01-08 10:03:40   \n",
       "3     2015-01-06 10:11:07  2015-01-07 09:37:30  2015-01-08 10:02:08   \n",
       "4     2015-01-06 09:45:28  2015-01-07 09:49:37  2015-01-08 10:19:44   \n",
       "...                   ...                  ...                  ...   \n",
       "4405  2015-01-06 10:26:51  2015-01-07 10:06:58  2015-01-08 09:45:06   \n",
       "4406  2015-01-06 09:44:00  2015-01-07 09:42:10  2015-01-08 10:00:57   \n",
       "4407  2015-01-06 09:49:17  2015-01-07 10:28:12  2015-01-08 09:47:38   \n",
       "4408  2015-01-06 10:12:50  2015-01-07 10:12:31  2015-01-08 09:42:57   \n",
       "4409  2015-01-06 09:52:30  2015-01-07 09:43:15  2015-01-08 10:06:55   \n",
       "\n",
       "               2015-01-09           2015-01-12           2015-01-13  ...  \\\n",
       "0     2015-01-09 10:09:25  2015-01-12 09:42:53  2015-01-13 10:13:06  ...   \n",
       "1     2015-01-09 09:43:26  2015-01-12 10:00:07  2015-01-13 10:43:29  ...   \n",
       "2     2015-01-09 10:05:49  2015-01-12 10:03:47  2015-01-13 10:21:26  ...   \n",
       "3     2015-01-09 10:08:12  2015-01-12 10:13:42  2015-01-13 09:53:22  ...   \n",
       "4     2015-01-09 10:00:50  2015-01-12 10:29:27  2015-01-13 09:59:32  ...   \n",
       "...                   ...                  ...                  ...  ...   \n",
       "4405  2015-01-09 09:49:24  2015-01-12 09:37:10  2015-01-13 09:25:02  ...   \n",
       "4406  2015-01-09 09:44:04  2015-01-12 10:07:32  2015-01-13 10:05:11  ...   \n",
       "4407  2015-01-09 10:01:03  2015-01-12 09:49:12  2015-01-13 09:47:10  ...   \n",
       "4408                  NaN  2015-01-12 10:00:38  2015-01-13 09:48:03  ...   \n",
       "4409  2015-01-09 10:27:39  2015-01-12 09:47:35  2015-01-13 09:30:00  ...   \n",
       "\n",
       "               2015-12-18           2015-12-21           2015-12-22  \\\n",
       "0                     NaN  2015-12-21 09:55:29  2015-12-22 10:04:06   \n",
       "1     2015-12-18 10:37:17  2015-12-21 09:49:02  2015-12-22 10:33:51   \n",
       "2     2015-12-18 10:15:14  2015-12-21 10:10:28  2015-12-22 09:44:44   \n",
       "3     2015-12-18 10:17:38  2015-12-21 09:58:21  2015-12-22 10:04:25   \n",
       "4     2015-12-18 09:58:35  2015-12-21 10:03:41  2015-12-22 10:10:30   \n",
       "...                   ...                  ...                  ...   \n",
       "4405  2015-12-18 10:01:06  2015-12-21 10:25:25  2015-12-22 10:16:11   \n",
       "4406  2015-12-18 09:27:32  2015-12-21 09:41:24  2015-12-22 09:50:30   \n",
       "4407  2015-12-18 10:00:57  2015-12-21 09:51:07  2015-12-22 10:02:10   \n",
       "4408  2015-12-18 09:54:33  2015-12-21 10:01:08  2015-12-22 10:10:19   \n",
       "4409  2015-12-18 10:20:40  2015-12-21 10:26:02                  NaN   \n",
       "\n",
       "               2015-12-23           2015-12-24 2015-12-25  \\\n",
       "0     2015-12-23 10:14:27  2015-12-24 10:11:35        NaN   \n",
       "1     2015-12-23 10:12:10                  NaN        NaN   \n",
       "2     2015-12-23 10:15:54  2015-12-24 10:07:26        NaN   \n",
       "3     2015-12-23 10:11:46  2015-12-24 09:43:15        NaN   \n",
       "4     2015-12-23 10:13:36  2015-12-24 09:44:24        NaN   \n",
       "...                   ...                  ...        ...   \n",
       "4405  2015-12-23 10:04:40  2015-12-24 09:45:40        NaN   \n",
       "4406  2015-12-23 10:32:21  2015-12-24 09:47:41        NaN   \n",
       "4407  2015-12-23 09:58:29  2015-12-24 09:56:05        NaN   \n",
       "4408  2015-12-23 09:42:30  2015-12-24 09:56:05        NaN   \n",
       "4409  2015-12-23 09:53:19  2015-12-24 09:42:13        NaN   \n",
       "\n",
       "               2015-12-28           2015-12-29           2015-12-30  \\\n",
       "0     2015-12-28 10:13:41  2015-12-29 10:03:36  2015-12-30 09:54:12   \n",
       "1     2015-12-28 09:31:45  2015-12-29 09:55:49  2015-12-30 10:32:25   \n",
       "2     2015-12-28 09:42:05  2015-12-29 09:43:36  2015-12-30 09:34:05   \n",
       "3     2015-12-28 09:52:44  2015-12-29 09:33:16  2015-12-30 10:18:12   \n",
       "4     2015-12-28 10:05:15  2015-12-29 10:30:53  2015-12-30 09:18:21   \n",
       "...                   ...                  ...                  ...   \n",
       "4405  2015-12-28 10:15:39  2015-12-29 10:10:09  2015-12-30 09:28:19   \n",
       "4406  2015-12-28 09:54:23  2015-12-29 10:13:32  2015-12-30 10:21:09   \n",
       "4407  2015-12-28 09:59:24                  NaN  2015-12-30 10:02:36   \n",
       "4408  2015-12-28 09:55:25  2015-12-29 09:54:42  2015-12-30 10:15:44   \n",
       "4409  2015-12-28 09:41:11  2015-12-29 10:05:15  2015-12-30 10:32:49   \n",
       "\n",
       "               2015-12-31  \n",
       "0     2015-12-31 10:12:44  \n",
       "1     2015-12-31 09:27:20  \n",
       "2     2015-12-31 10:28:39  \n",
       "3     2015-12-31 10:01:15  \n",
       "4     2015-12-31 09:41:09  \n",
       "...                   ...  \n",
       "4405  2015-12-31 10:00:12  \n",
       "4406  2015-12-31 10:09:48  \n",
       "4407  2015-12-31 10:03:30  \n",
       "4408  2015-12-31 09:56:47  \n",
       "4409  2015-12-31 10:09:28  \n",
       "\n",
       "[4410 rows x 262 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./data/\"\n",
    "\n",
    "general_data = pd.read_csv(path + \"general_data.csv\")\n",
    "employe_data = pd.read_csv(path + \"employee_survey_data.csv\")\n",
    "manager_data = pd.read_csv(path + \"manager_survey_data.csv\")\n",
    "In_data = pd.read_csv(path + \"in_time.csv\")\n",
    "Out_data = pd.read_csv(path + \"out_time.csv\")\n",
    "\n",
    "general = general_data.copy()\n",
    "employe = employe_data.copy()\n",
    "manager = manager_data.copy()\n",
    "In = In_data.copy()\n",
    "Out = Out_data.copy()\n",
    "In"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aaf9d5-413e-4ea1-8ae9-ecf856d825a3",
   "metadata": {},
   "source": [
    "## Choix des données \n",
    "\n",
    "### Éthique\n",
    "\n",
    "Dans le cas de notre modèle et de l'utilisation de nos données il est important de déterminer éthiquement la conservation de certaines données ou non. \n",
    "\n",
    "Pour cela nous avons procédé à un brainstorming et à une lecture des recommendations de la CNIL pour conserver uniquement les données nécessaires et ne prêtant pas à une possible discrimination.\n",
    "\n",
    "Voici la liste des données que nous ne souhaitons pas conserver pour notre modèle :\n",
    "\n",
    "- **L’âge des employés (`Age`) :** Nous souhaitons rester dans la plus grande neutralité possible. Effectivement, l’âge ne doit pas nous permettre de définir si une personne est plus à même de quitter l’entreprise ou non. \n",
    "<br>\n",
    "\n",
    "- **Le genre des employés (`Gender`) :** Le genre est une donnée non pertinente sur les critères qui pousserai à un turnover. Cette donnée pourrait être discriminante. \n",
    "<br>\n",
    "\n",
    "- **Le statut marital (`MaritalStatus`) :** Cette donnée ne nous permettrai pas d’interpreter des critères cohérent concernant les Turn-over dans l’entreprise. Ce serai une surinterprétation des données fournis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "086131f1-51f5-4e28-9a5c-0082412d0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the columns for ethic\n",
    "general = general.drop(columns=['Age','Gender','MaritalStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33907ac6-3edb-4412-9680-006af556d0f6",
   "metadata": {},
   "source": [
    "### Données inutiles\n",
    "\n",
    "Il est crucial d'adopter une approche méthodique pour identifier les données pertinentes à conserver dans notre analyse.\r\n",
    "\r\n",
    "Dans cette optique, nous commencerons par cibler les colonnes du tableau General_data où chaque entrée présente une valeur uniforme, c'est-à-dire égale à 1. Cela signifie que tous les enregistrements dans ces colonnes sont identiques.\r\n",
    "\r\n",
    "Pour repérer ces colonnes spécifiques, nous pouvons exécuter une fonction dédiée. Cette étape nous permettra de déterminer avec précision les champs qui remplissent ce critère."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df406db0-b306-46de-b7dc-a2687499d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmployeeCount\n",
      "[1]\n",
      "Over18\n",
      "['Y']\n",
      "StandardHours\n",
      "[8]\n"
     ]
    }
   ],
   "source": [
    "for col in general.columns:\n",
    "    value = general[col].nunique()\n",
    "    value_of = general[col].unique()\n",
    "    if value == 1:\n",
    "        print(col)\n",
    "        print(value_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86021a9e-1a19-454a-80c6-57fcf604d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "general = general.drop(columns=['EmployeeCount','Over18'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524852f-3419-4b12-b196-2b37f9fea46b",
   "metadata": {},
   "source": [
    "Nous avons opté pour l'élimination de la colonne EmployeeCount de notre jeu de données, car cette colonne présente une valeur unique pour l'ensemble du dataset, ce qui la rend non pertinente pour notre analyse.\n",
    "\n",
    "Nous enlevons aussi Over18Y, car tout les employés sont majeurs, comme l'impose la loi.\n",
    "\n",
    "Nous gardons StandardHours car nous l'utiliserons plus tard pour classer les emplyyés par quanité de travail.\n",
    "\n",
    "### Conclusion choix des données \n",
    "\n",
    "Après avoir sélectionné les données à conserver en fonction de différents facteurs voici une liste exhaustive de celles conservées pour la réalisation de notre modèle :\n",
    "\n",
    "\n",
    "<font color='blue'> General_data </font>\n",
    "\n",
    "- **Attrition :** L'objet de notre étude, est-ce que l'employé a quitté l'entreprise durant l'année 2016 ?\n",
    "<br>\n",
    "\n",
    "- **BusinessTravel :** A quel fréquence l'employé a été amené à se déplacer dans le cadre de son travail en 2015 ? (Non-Travel = jamais, Travel_Rarely= rarement, Travel_Frequently = fréquemment)\n",
    "<br>\n",
    "\n",
    "- **DistanceFromHome :** Distance en km entre le logement de l'employé et l'entreprise.\n",
    "<br>\n",
    "\n",
    "- **Education : Niveau d'étude :** 1=Avant College (équivalent niveau Bac), 2=College (équivalent Bac+2), 3=Bachelor (Bac+3), 4=Master (Bac+5) et 5=PhD (Thèse de doctorat).\n",
    "<br>\n",
    "\n",
    "- **EducationField :** Domaine d'étude, matière principale\n",
    "<br>\n",
    "\n",
    "- **EmployeeId :** l'identifiant d'un employé\n",
    "<br>\n",
    "\n",
    "- **JobLevel :** Niveau hiérarchique dans l'entreprise de 1 à 5\n",
    "<br>\n",
    "\n",
    "- **JobRole :** Métier dans l'entreprise\n",
    "<br>\n",
    "\n",
    "- **MonthlyIncome :** Salaire brut en roupies par mois\n",
    "<br>\n",
    "\n",
    "- **NumCompaniesWorked :** Nombre d'entreprises pour lequel le salarié a travaillé avant de rejoindre HumanForYou.\n",
    "<br>\n",
    "\n",
    "- **PercentSalaryHike :** % d'augmentation du salaire en 2015.\n",
    "<br>\n",
    "\n",
    "- **StockOptionLevel :** Niveau d'investissement en actions de l'entreprise par le salarié.\n",
    "<br>\n",
    "\n",
    "- **TotalWorkingYears :** Nombre d'années d'expérience en entreprise du salarié pour le même type de poste.\n",
    "<br>\n",
    "\n",
    "- **TrainingTimesLastYear :** Nombre de jours de formation en 2015\n",
    "<br>\n",
    "\n",
    "- **YearsAtCompany :** Ancienneté dans l'entreprise\n",
    "<br>\n",
    "\n",
    "- **YearsSinceLastPromotion :** Nombre d'années depuis la dernière augmentation individuelle\n",
    "<br>\n",
    "\n",
    "- **YearsWithCurrentManager :** Nombre d'années de collaboration sous la responsabilité du manager actuel de l'employé.\n",
    "\n",
    "<font color='blue'> Employee_survey_data </font>\n",
    "\n",
    "- **L'environnement de travail :** : noté 1 (\"Faible\"), 2 (\"Moyen\"), 3 (\"Élevé\") ou 4 (\"Très élevé\") : EnvironmentSatisfaction\n",
    "<br>\n",
    "\n",
    "- **Son travail :** noté de 1 à 4 comme précédemment : JobSatisfaction\n",
    "<br>\n",
    "\n",
    "- **Son équilibre entre vie professionnelle et vie privée :** noté 1 (\"Mauvais\"), 2 (\"Satisfaisant\"), 3 (\"Très satisfaisant\") ou 4 (\"Excellent\") : WorkLifeBalance\n",
    "\n",
    "<font color='blue'> Manager_survey_data </font>\n",
    "\n",
    "- **Une évaluation de son implication dans son travail :** notée 1 ('Faible'), 2 (\"Moyenne\"), 3 (\"Importante\") ou 4 (\"Très importante\") : JobInvolvement\n",
    "<br>\n",
    "\n",
    "- **Une évaluation de son niveau de performance annuel pour l'entreprise :** notée 1 (\"Faible\"), 2 (\"Bon\"), 3 (\"Excellent\") ou 4 (\"Au delà des attentes\") : PerformanceRating\n",
    "\n",
    "<font color='blue'> In_Time et Out_Time </font>\n",
    "\n",
    "Nous allons conserver toutes les données de badgeuse d'entrée et de sortie afin d'effectuer différents statistiques sur ces données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70ab3b-eb43-43f6-988b-2922eabc11e8",
   "metadata": {},
   "source": [
    "## Manipulation des fichiers des horaires de travail\n",
    "\n",
    "Nous allons travailler sur les fichiers concernant les horaires de travail afin de les rendre utilisables.\n",
    "\n",
    "D'abord, nous changeons les NaN en 0 (ce qui correspond donc aux jours de congés) puis on vérifie que le nombre de jours de congés est bien le même dans les deux tables. On affiche ensuite un histogramme afin d'en retirer un aperçu plus visuel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e525e53-3832-46f8-9a1d-924acbfd4758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "In.fillna(0, inplace=True)\n",
    "Out.fillna(0, inplace=True)\n",
    "vac_In = (In == 0).astype(int).sum(axis=1)\n",
    "vac_Out = (Out ==0).astype(int).sum(axis=1)\n",
    "\n",
    "vac_In.equals(vac_Out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5712088-2706-44b0-b610-a17a0ba9b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vac_In.hist(bins=50, figsize=(20,15))\n",
    "#plt.xlabel(\"Nombre de jours de congés\")\n",
    "#plt.ylabel(\"Nombre d'employés\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d2fa55-ed75-40e6-92cf-a718039e5589",
   "metadata": {},
   "source": [
    "On supprime la première colonne des deux tableaux car elles sont inutiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b4c845-26aa-4401-bc4a-8283a6f2d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "In = In.drop(In.columns[0], axis = 1)\n",
    "Out = Out.drop(Out.columns[0], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3bbe5-dd66-40e7-b9ab-6c46a137a494",
   "metadata": {},
   "source": [
    "Nous avons traité et analysé les données relatives aux horaires d'arrivée et de départ des employés en convertissant les données en format datetime, en gérant les erreurs pour remplacer les valeurs inappropriées par NaT. \n",
    "\n",
    "Nous avons défini deux fonctions essentielles : mean_angle pour calculer la moyenne des angles, facilitant la moyenne des temps en considérant ces derniers comme des vecteurs sur un cercle de 24 heures, et mean_time pour déterminer le temps moyen d'arrivée ou de départ, en excluant les valeurs manquantes. Cette dernière convertit les temps en secondes, calcule leur moyenne angulaire pour gérer correctement le passage de minuit, puis reconvertit cette moyenne en heures, minutes et secondes. Nous avons ensuite appliqué mean_time aux ensembles de données d'arrivée et de départ pour obtenir l'heure moyenne d'arrivée (AverageInTime) et de départ (AverageOutTime), nous permettant d'avoir une vue d'ensemble précise des tendances horaires des employés, essentielle pour comprendre leur comportement de travail et identifier d'éventuels liens avec la satisfaction ou le taux de rotation.\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d1180a5-0695-4656-82b1-dfa4a49b61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "In_h = In.iloc[:, 0:262].apply(lambda x: pd.to_datetime(x, format='%Y-%m-%d %H:%M:%S', errors='coerce'))\n",
    "Out_h = Out.iloc[:, 0:262].apply(lambda x: pd.to_datetime(x, format='%Y-%m-%d %H:%M:%S', errors='coerce'))\n",
    "\n",
    "def mean_angle(deg):\n",
    "    return degrees(phase(sum(rect(1, radians(d)) for d in deg)/len(deg)))\n",
    "\n",
    "def mean_time(row):\n",
    "    row = row.dropna()  # Supprimer les valeurs NaT\n",
    "    if row.empty:\n",
    "        return None  # Retourner None ou une valeur par défaut si la ligne est vide après la suppression des NaT\n",
    "    t = ([time.hour, time.minute, time.second] for time in row)\n",
    "    seconds = ((float(s) + int(m) * 60 + int(h) * 3600) for h, m, s in t)\n",
    "    seconds = [i for i in seconds if i > 0]\n",
    "    if not seconds:\n",
    "        return None  # Retourner None ou une valeur par défaut si aucun second n'est > 0\n",
    "    day = 24 * 60 * 60\n",
    "    to_angles = [s * 360. / day for s in seconds]\n",
    "    mean_as_angle = mean_angle(to_angles)\n",
    "    mean_seconds = mean_as_angle * day / 360.\n",
    "    if mean_seconds < 0:\n",
    "        mean_seconds += day\n",
    "    h, m = divmod(mean_seconds, 3600)\n",
    "    m, s = divmod(m, 60)\n",
    "    return '%02i:%02i:%02i' % (h, m, s)\n",
    "\n",
    "In_h['AverageInTime'] = In_h.apply(mean_time, axis=1)\n",
    "Out_h['AverageOutTime'] = Out_h.apply(mean_time, axis=1)  # Correction du nom de colonne pour AverageOutTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1592467-f969-4be9-a7d2-86ff32a86e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   NaN\n",
       "1   NaN\n",
       "2   NaN\n",
       "3   NaN\n",
       "4   NaN\n",
       "Name: AverageInTime, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "In_h['AverageInTime'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d548c-220f-4e4a-9e77-c25f555b3281",
   "metadata": {},
   "source": [
    "Nous calculons ensuite la durée moyenne de travail quotidien des employés en soustrayant l'heure moyenne d'arrivée (Average_In) de l'heure moyenne de départ (Average_Out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14def2e3-8ddb-4e22-bacd-ec5b20821a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      NaT\n",
      "1      NaT\n",
      "2      NaT\n",
      "3      NaT\n",
      "4      NaT\n",
      "        ..\n",
      "4405   NaT\n",
      "4406   NaT\n",
      "4407   NaT\n",
      "4408   NaT\n",
      "4409   NaT\n",
      "Length: 4410, dtype: timedelta64[ns]\n"
     ]
    }
   ],
   "source": [
    "Average_Out = pd.to_timedelta(Out_h['AverageOutTime'])\n",
    "Average_In = pd.to_timedelta(In_h['AverageInTime'])\n",
    "\n",
    "AverageTimeWorking = (Average_Out - Average_In)\n",
    "\n",
    "print(AverageTimeWorking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2791182-00c9-4211-a7f9-3e9dce55af11",
   "metadata": {},
   "source": [
    "## Fusion des données\n",
    "\n",
    "On peut maintenant fusionner les csv restant et ajouter deux nouvelles colonnes : la première correspondant au nombre de jours de congés pris par l'employé, et la seconde au temps de travail moyen par jour de l'employé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c565143-d681-4888-a97e-df229b830c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>JobLevel</th>\n",
       "      <th>JobRole</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>...</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "      <th>JobInvolvement</th>\n",
       "      <th>PerformanceRating</th>\n",
       "      <th>EnvironmentSatisfaction</th>\n",
       "      <th>JobSatisfaction</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>AverageTimeWorking</th>\n",
       "      <th>Holidays</th>\n",
       "      <th>AverageInTime</th>\n",
       "      <th>AverageOutTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>Sales</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Healthcare Representative</td>\n",
       "      <td>131160</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>41890</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>Other</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>193280</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>Non-Travel</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Human Resources</td>\n",
       "      <td>83210</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>23420</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4405</th>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Medical</td>\n",
       "      <td>4406</td>\n",
       "      <td>1</td>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>60290</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4406</th>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Medical</td>\n",
       "      <td>4407</td>\n",
       "      <td>1</td>\n",
       "      <td>Laboratory Technician</td>\n",
       "      <td>26790</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4407</th>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>4408</td>\n",
       "      <td>2</td>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>37020</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4408</th>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>Sales</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>Medical</td>\n",
       "      <td>4409</td>\n",
       "      <td>1</td>\n",
       "      <td>Laboratory Technician</td>\n",
       "      <td>23980</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4409</th>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>Medical</td>\n",
       "      <td>4410</td>\n",
       "      <td>2</td>\n",
       "      <td>Laboratory Technician</td>\n",
       "      <td>54680</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4410 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Attrition     BusinessTravel              Department  DistanceFromHome  \\\n",
       "0           No      Travel_Rarely                   Sales                 6   \n",
       "1          Yes  Travel_Frequently  Research & Development                10   \n",
       "2           No  Travel_Frequently  Research & Development                17   \n",
       "3           No         Non-Travel  Research & Development                 2   \n",
       "4           No      Travel_Rarely  Research & Development                10   \n",
       "...        ...                ...                     ...               ...   \n",
       "4405        No      Travel_Rarely  Research & Development                 5   \n",
       "4406        No      Travel_Rarely  Research & Development                 2   \n",
       "4407        No      Travel_Rarely  Research & Development                25   \n",
       "4408        No      Travel_Rarely                   Sales                18   \n",
       "4409        No      Travel_Rarely  Research & Development                28   \n",
       "\n",
       "      Education EducationField  EmployeeID  JobLevel  \\\n",
       "0             2  Life Sciences           1         1   \n",
       "1             1  Life Sciences           2         1   \n",
       "2             4          Other           3         4   \n",
       "3             5  Life Sciences           4         3   \n",
       "4             1        Medical           5         1   \n",
       "...         ...            ...         ...       ...   \n",
       "4405          4        Medical        4406         1   \n",
       "4406          4        Medical        4407         1   \n",
       "4407          2  Life Sciences        4408         2   \n",
       "4408          2        Medical        4409         1   \n",
       "4409          3        Medical        4410         2   \n",
       "\n",
       "                        JobRole  MonthlyIncome  ...  YearsWithCurrManager  \\\n",
       "0     Healthcare Representative         131160  ...                     0   \n",
       "1            Research Scientist          41890  ...                     4   \n",
       "2               Sales Executive         193280  ...                     3   \n",
       "3               Human Resources          83210  ...                     5   \n",
       "4               Sales Executive          23420  ...                     4   \n",
       "...                         ...            ...  ...                   ...   \n",
       "4405         Research Scientist          60290  ...                     2   \n",
       "4406      Laboratory Technician          26790  ...                     2   \n",
       "4407            Sales Executive          37020  ...                     2   \n",
       "4408      Laboratory Technician          23980  ...                     8   \n",
       "4409      Laboratory Technician          54680  ...                     9   \n",
       "\n",
       "      JobInvolvement  PerformanceRating  EnvironmentSatisfaction  \\\n",
       "0                  3                  3                      3.0   \n",
       "1                  2                  4                      3.0   \n",
       "2                  3                  3                      2.0   \n",
       "3                  2                  3                      4.0   \n",
       "4                  3                  3                      4.0   \n",
       "...              ...                ...                      ...   \n",
       "4405               3                  3                      4.0   \n",
       "4406               2                  3                      4.0   \n",
       "4407               3                  4                      1.0   \n",
       "4408               2                  3                      4.0   \n",
       "4409               4                  3                      1.0   \n",
       "\n",
       "      JobSatisfaction  WorkLifeBalance  AverageTimeWorking  Holidays  \\\n",
       "0                 4.0              2.0                 NaT         0   \n",
       "1                 2.0              4.0                 NaT         0   \n",
       "2                 2.0              1.0                 NaT         0   \n",
       "3                 4.0              3.0                 NaT         0   \n",
       "4                 1.0              3.0                 NaT         0   \n",
       "...               ...              ...                 ...       ...   \n",
       "4405              1.0              3.0                 NaT         0   \n",
       "4406              4.0              3.0                 NaT         0   \n",
       "4407              3.0              3.0                 NaT         0   \n",
       "4408              1.0              3.0                 NaT         0   \n",
       "4409              3.0              0.0                 NaT         0   \n",
       "\n",
       "      AverageInTime  AverageOutTime  \n",
       "0               NaT             NaT  \n",
       "1               NaT             NaT  \n",
       "2               NaT             NaT  \n",
       "3               NaT             NaT  \n",
       "4               NaT             NaT  \n",
       "...             ...             ...  \n",
       "4405            NaT             NaT  \n",
       "4406            NaT             NaT  \n",
       "4407            NaT             NaT  \n",
       "4408            NaT             NaT  \n",
       "4409            NaT             NaT  \n",
       "\n",
       "[4410 rows x 28 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = general.merge(manager, how='right', on = 'EmployeeID')\n",
    "combined_csv = combined.merge(employe, how='right', on = 'EmployeeID')\n",
    "#On transforme tous les NA en 0\n",
    "combined_csv.fillna(0, inplace=True)\n",
    "combined_csv['AverageTimeWorking'] = AverageTimeWorking\n",
    "combined_csv['Holidays'] = vac_In\n",
    "\n",
    "combined_csv['AverageInTime'] = Average_In\n",
    "combined_csv['AverageOutTime'] = Average_Out\n",
    "\n",
    "combined_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab7305-7a58-4f65-be5f-9a06d9f4223a",
   "metadata": {},
   "source": [
    "On remarque que le temps de travail moyen n'est pas dans un format manipulable, nous allons donc le transofrmer en float. Idem pour l'heure d'arrivée et de départ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16d5d8ec-103b-42a5-801c-17b4e61c58c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      NaN\n",
      "1      NaN\n",
      "2      NaN\n",
      "3      NaN\n",
      "4      NaN\n",
      "        ..\n",
      "4405   NaN\n",
      "4406   NaN\n",
      "4407   NaN\n",
      "4408   NaN\n",
      "4409   NaN\n",
      "Name: AverageTimeWorking, Length: 4410, dtype: float64\n",
      "0      NaN\n",
      "1      NaN\n",
      "2      NaN\n",
      "3      NaN\n",
      "4      NaN\n",
      "        ..\n",
      "4405   NaN\n",
      "4406   NaN\n",
      "4407   NaN\n",
      "4408   NaN\n",
      "4409   NaN\n",
      "Name: AverageInTime, Length: 4410, dtype: float64\n",
      "0      NaN\n",
      "1      NaN\n",
      "2      NaN\n",
      "3      NaN\n",
      "4      NaN\n",
      "        ..\n",
      "4405   NaN\n",
      "4406   NaN\n",
      "4407   NaN\n",
      "4408   NaN\n",
      "4409   NaN\n",
      "Name: AverageOutTime, Length: 4410, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "Time_float = combined_csv['AverageTimeWorking'].dt.seconds\n",
    "Time_float = Time_float/3600\n",
    "print(Time_float)\n",
    "combined_csv['AverageTimeWorking'] = Time_float\n",
    "\n",
    "Time_float = combined_csv['AverageInTime'].dt.seconds\n",
    "Time_float = Time_float/3600\n",
    "print(Time_float)\n",
    "combined_csv['AverageInTime'] = Time_float\n",
    "\n",
    "Time_float = combined_csv['AverageOutTime'].dt.seconds\n",
    "Time_float = Time_float/3600\n",
    "print(Time_float)\n",
    "combined_csv['AverageOutTime'] = Time_float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba3365-747c-44cb-8e60-2a0cead9144d",
   "metadata": {},
   "source": [
    "Nous pouvons utiliser la méthode .info() sur notre datafrae pour obtenir un résumé concis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acb0903e-fb88-4396-a79b-c8ec94cc5c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4410 entries, 0 to 4409\n",
      "Data columns (total 28 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Attrition                4410 non-null   object \n",
      " 1   BusinessTravel           4410 non-null   object \n",
      " 2   Department               4410 non-null   object \n",
      " 3   DistanceFromHome         4410 non-null   int64  \n",
      " 4   Education                4410 non-null   int64  \n",
      " 5   EducationField           4410 non-null   object \n",
      " 6   EmployeeID               4410 non-null   int64  \n",
      " 7   JobLevel                 4410 non-null   int64  \n",
      " 8   JobRole                  4410 non-null   object \n",
      " 9   MonthlyIncome            4410 non-null   int64  \n",
      " 10  NumCompaniesWorked       4410 non-null   float64\n",
      " 11  PercentSalaryHike        4410 non-null   int64  \n",
      " 12  StandardHours            4410 non-null   int64  \n",
      " 13  StockOptionLevel         4410 non-null   int64  \n",
      " 14  TotalWorkingYears        4410 non-null   float64\n",
      " 15  TrainingTimesLastYear    4410 non-null   int64  \n",
      " 16  YearsAtCompany           4410 non-null   int64  \n",
      " 17  YearsSinceLastPromotion  4410 non-null   int64  \n",
      " 18  YearsWithCurrManager     4410 non-null   int64  \n",
      " 19  JobInvolvement           4410 non-null   int64  \n",
      " 20  PerformanceRating        4410 non-null   int64  \n",
      " 21  EnvironmentSatisfaction  4410 non-null   float64\n",
      " 22  JobSatisfaction          4410 non-null   float64\n",
      " 23  WorkLifeBalance          4410 non-null   float64\n",
      " 24  AverageTimeWorking       0 non-null      float64\n",
      " 25  Holidays                 4410 non-null   int64  \n",
      " 26  AverageInTime            0 non-null      float64\n",
      " 27  AverageOutTime           0 non-null      float64\n",
      "dtypes: float64(8), int64(15), object(5)\n",
      "memory usage: 964.8+ KB\n"
     ]
    }
   ],
   "source": [
    "combined_csv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0aff4f-ab7e-445f-8df5-d01c6d8f8ec7",
   "metadata": {},
   "source": [
    "Pour notre projet, il serait judicieux d'examiner si certains employés sont surchargés de travail tandis que d'autres ont une charge de travail moins importante. Nous pourrions enrichir notre analyse en ajoutant une colonne qui catégoriserait chaque employé selon sa charge de travail : normale, faible, ou excessive. Cela apporterait une dimension supplémentaire à notre étude sur les facteurs influençant le taux de rotation au sein de l'entreprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fad528b6-f92e-49c3-b38c-901cbe4afed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    0.0\n",
       "mean     NaN\n",
       "std      NaN\n",
       "min      NaN\n",
       "25%      NaN\n",
       "50%      NaN\n",
       "75%      NaN\n",
       "max      NaN\n",
       "Name: AverageTimeWorking, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_csv['AverageTimeWorking'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4e431-db69-4526-a255-da8bd87bbd77",
   "metadata": {},
   "source": [
    "Les données révèlent en effet des disparités significatives dans le temps de travail moyen des employés. Certains affichent un temps de travail moyen inférieur à 7 heures, ce qui peut être considéré comme relativement faible. À l'opposé, d'autres employés dépassent les 8 heures de travail moyen, atteignant même jusqu'à 11 heures, ce qui suggère une situation de surmenage.\n",
    "\n",
    "Nous allons élaborer une fonction destinée à générer une liste peuplée de booléens, où le chiffre 0 indiquera que l'employé travaille moins que la norme, 1 signifiera que son volume de travail est standard, et 2 révélera un état de surmenage.\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1ee4c8d-5658-47d3-8167-f3dee7f9bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WorkingTimeEval = []\n",
    "for i in range (0, combined_csv.shape[0]):\n",
    "    if combined_csv['AverageTimeWorking'][i] < combined_csv['StandardHours'][i] :\n",
    "        WorkingTimeEval.append(0)\n",
    "    elif (combined_csv['AverageTimeWorking'][i] >= combined_csv['StandardHours'][i]) and (combined_csv['AverageTimeWorking'][i] <= 9):\n",
    "        WorkingTimeEval.append(1)\n",
    "    elif combined_csv['AverageTimeWorking'][i] > 9 :\n",
    "        WorkingTimeEval.append(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f49cc-a9fa-4586-ad16-23eaab5af981",
   "metadata": {},
   "source": [
    "Maintenant que notre liste est complète, nous pouvons l'intégrer comme nouvelle colonne dans notre ensemble de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a07ffbd8-39c6-4675-b4f5-16042b462d98",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (0) does not match length of index (4410)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcombined_csv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWorkingTimeEval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m WorkingTimeEval\n\u001b[0;32m      2\u001b[0m combined_csv\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4299\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4298\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4512\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4503\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4504\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4505\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4510\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4511\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4512\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4515\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4516\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4517\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4518\u001b[0m     ):\n\u001b[0;32m   4519\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4520\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:5253\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5253\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5254\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5256\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5257\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5260\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5261\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\pandas\\core\\common.py:571\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (0) does not match length of index (4410)"
     ]
    }
   ],
   "source": [
    "combined_csv['WorkingTimeEval'] = WorkingTimeEval\n",
    "combined_csv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad48000-0953-4c03-99fd-666585c9802c",
   "metadata": {},
   "source": [
    "## Exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641164d-84e3-4352-982f-b5ac9a32e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3a22ba-7850-4f26-bacb-e6630a00ff2b",
   "metadata": {},
   "source": [
    "## Pipeline de transformation\n",
    "\n",
    "Dans ce code, nous avons entrepris la préparation des données pour un modèle de machine learning, en identifiant d'abord les caractéristiques numériques et catégorielles nécessaires à la transformation. Les caractéristiques numériques (num_features) comprennent des variables telles que l'âge, le revenu mensuel, la distance domicile-travail, les années de travail totales, entre autres, tandis que les caractéristiques catégorielles (cat_features) incluent le voyage d'affaires, le département, le domaine d'éducation, etc.\r\n",
    "\r\n",
    "Nous avons ensuite créé des pipelines séparés pour le traitement des caractéristiques numériques et catégorielles. Le pipeline numérique (num_pipeline) utilise un imputateur pour remplacer les valeurs manquantes par la moyenne et un normalisateur pour standardiser les caractéristiques. Le pipeline catégoriel (cat_pipeline), quant à lui, remplace les valeurs manquantes par la valeur la plus fréquente et applique un encodage one-hot pour gérer les variables catégorielles.\r\n",
    "\r\n",
    "Ces pipelines sont combinés dans un transformateur de colonnes (ColumnTransformer), formant un préprocesseur complet qui effectue simultanément les transformations numériques et catégorielles sur les données.\r\n",
    "\r\n",
    "Le dataset est ensuite divisé en caractéristiques (X) et cible (y), avec y étant binarisée pour refléter l'attrition comme une variable binaire (1 pour 'Yes', 0 pour 'No').\r\n",
    "\r\n",
    "Nous vérifions enfin l'existence des colonnes référencées dans X pour s'assurer que toutes les colonnes nécessaires sont présentes, en affichant les colonnes manquantes le cas échéant. Cette étape est cruciale pour éviter les erreurs lors de l'application du préprocesseur sur le dataset, garantissant que les transformations sont appliquées correctement à toutes les caractéristiques spécifiées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145a0fe-39de-4c09-a8ad-2c449d7fd7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des colonnes pour chaque transformation\n",
    "num_features = ['MonthlyIncome', 'DistanceFromHome', 'TotalWorkingYears', 'NumCompaniesWorked', 'YearsAtCompany', 'Holidays', 'AverageTimeWorking', 'AverageInTime', 'AverageOutTime', 'WorkingTimeEval']\n",
    "cat_features = ['BusinessTravel','Department' , 'EducationField', 'JobRole']\n",
    "\n",
    "# Création des pipelines pour les transformations numériques et catégorielles\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Assemblage du pipeline complet\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "])\n",
    "\n",
    "# Division du dataset\n",
    "X = combined_csv.drop('Attrition', axis=1)\n",
    "y = combined_csv['Attrition'].apply(lambda x: 1 if x == 'Yes' else 0)  # Conversion de la cible en binaire\n",
    "\n",
    "\n",
    "# Vérifiez si toutes les colonnes référencées existent dans X\n",
    "print(X.columns)\n",
    "missing_cols_num = [col for col in num_features if col not in X.columns]\n",
    "missing_cols_cat = [col for col in cat_features if col not in X.columns]\n",
    "print(\"Colonnes numériques manquantes:\", missing_cols_num)\n",
    "print(\"Colonnes catégorielles manquantes:\", missing_cols_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d4da4-24f9-4da6-822e-e9f022232437",
   "metadata": {},
   "source": [
    "## Algorithmes de Machine Learning \n",
    "Notre pipeline de prétraitement des données est désormais prêt, intégrant soigneusement les transformations nécessaires pour les caractéristiques numériques et catégorielles. Cette étape cruciale nous permet d'assurer que les données sont correctement formatées et normalisées pour l'application des modèles d'apprentissage automatique. Avec cette préparation achevée, nous pouvons maintenant passer à la phase suivante de notre projet : la sélection et l'application des algorithmes de machine learning. \n",
    "\n",
    "## Méthode des k plus proches voisins (KNN)\n",
    "La méthode des k plus proches voisins (KNN) est un algorithme d'apprentissage supervisé simple et intuitif utilisé pour la classification et la régression. Le principe de base de KNN est de trouver les k échantillons les plus proches (ou voisins) d'un point de données non classifié dans l'espace des caractéristiques et de prédire son étiquette (pour la classification) ou sa valeur (pour la régression) en se basant sur la majorité ou la moyenne des étiquettes ou valeurs de ces voisins. La distance entre les points de données, souvent calculée par la distance euclidienne, sert à déterminer les voisins les plus proches. L'efficacité de KNN dépend du choix de k (le nombre de voisins) et de la mesure de distance utilisée. Bien qu'il soit facile à comprendre et à mettre en œuvre, KNN peut devenir inefficace en termes de temps et de mémoire avec de grands ensembles de données, car il nécessite de calculer la distance à chaque point de données lors de la prédiction.\n",
    "\n",
    "### Copie des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3bc74-fc72-4329-8ff6-178a5d96d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetKNN = combined_csv.copy()\n",
    "dataSetKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c9adb-4fb8-4973-b6f9-dcc9a12aed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un pipeline intégrant le préprocesseur et le modèle KNN\n",
    "knn_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992fcc5-7063-4dc0-bddf-63beca2598ba",
   "metadata": {},
   "source": [
    "### Entraienemnt du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff4d543-d17c-4000-a6b2-4f554f990834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entraînement du pipeline\n",
    "knn_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0986f0fe-9361-4657-ad77-58c8cfefa840",
   "metadata": {},
   "source": [
    "### Prédicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf4393-6ca3-4dc7-870b-b443693ab455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions sur l'ensemble de test\n",
    "y_pred = knn_pipeline.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bafba4-0f49-4b57-96c0-eac2b9d80ce9",
   "metadata": {},
   "source": [
    "Les résultats obtenus avec l'algorithme des k plus proches voisins (KNN) pour notre projet de prédiction de l'attrition des employés indiquent une précision globale de 86.3%. Cette précision reflète la proportion d'employés correctement classés par le modèle par rapport à l'ensemble total des prédictions.\r\n",
    "\r\n",
    "Dans le détail du rapport de classification, nous observons des performances variables entre les classes. Pour la classe 0 (employés restants), le modèle affiche une précision de889%, un rappel de796%, et un score F1 de 92%, indiquant une performance élevée dans l'identification des employés qui ne quittent pas l'entreprise. Cela suggère que le modèle est particulièrement efficace pour reconnaître les cas de non-attrition.\r\n",
    "\r\n",
    "Cependant, pour la classe 1 (employés quittant l'entreprise), les performances sont nettement inférieures : une précision 3e 62%, un rappel 1e 35%, et un score F1 1e 44%. Ces résultats montrent que, bien que le modèle soit relativement précis lorsqu'il prédit une attrition, il a tendance à ne reconnaître qu'une faible proportion des cas réels d'attrition (faible rappel), ce qui limite son efficacité pour identifier les employés à risque de dépnnées.\r\n",
    "\r\n",
    "En résumé, bien que le modèle KNN montre une précision globale élevée, l'analyse détaillée révèle des limites, notamment dans la détection de l'attrition des employés. Cela souligne l'importance de considérer d'autres modèles ou d'ajuster les paramètres du KNN pour améliorer la sensibilité du modèle aux cas d'attrition.\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed87c6-0d8f-4267-81c9-011521d47ff4",
   "metadata": {},
   "source": [
    "### Matrice de confusion\n",
    "\n",
    "La matrice de confusion montre les prédictions correctes et incorrectes du modèle en détail. Elle est particulièrement utile pour voir comment le modèle performe pour chaque classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d614b-e185-447a-9fbc-ebf6294e1b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)  # Utilisez y_pred_best si vous avez fait de l'optimisation hyperparamètre\n",
    "# Ou utilisez y_pred pour le modèle sans optimisation\n",
    "\n",
    "# Visualisation de la matrice de confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non', 'Oui'], yticklabels=['Non', 'Oui'])\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Valeurs Réelles')\n",
    "plt.title('Matrice de Confusion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e1a18-c6aa-4fe8-ac60-45cec12680c8",
   "metadata": {},
   "source": [
    "La matrice montre les résultats suivants :\n",
    "- Vrais Négatifs (TN) : 1077 employés ont été correctement prédits comme ne quittant pas l'entreprise.\n",
    "- Faux Positifs (FP) : 38 employés ont été incorrectement prédits comme quittant l'entreprise alors qu'ils sont restés.\n",
    "- Faux Négatifs (FN) : 144 employés ont été incorrectement prédits comme restant alors qu'ils ont quitté l'entreprise.\n",
    "- Vrais Positifs (VP) : 64 employés ont été correctement prédits comme quittant l'entreprise.\n",
    "\n",
    "Le modèle a une forte tendance à correctement identifier les employés qui restent (TN élevé), mais il est moins performant pour identifier correctement ceux qui quittent l'entreprise (VP plus faible par rapport au FN). Le nombre élevé de FN indique que le modèle pourrait être amélioré pour mieux détecter les cas d'attrition. Les FP relativement bas indiquent que le modèle ne fait pas beaucoup d'erreurs en classant faussement les employés comme partants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725dbef-dcdf-4d6c-ad0d-0594bfbff32c",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "\n",
    "La courbe Receiver Operating Characteristic (ROC) et l'Area Under the Curve (AUC) évaluent la performance du modèle dans les tâches de classification binaire à différents seuils. Ces métriques sont utiles pour évaluer la capacité du modèle à distinguer entre les classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81426b-1432-496a-a530-c502555f7f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des taux de vrais positifs et faux positifs\n",
    "fpr, tpr, thresholds = roc_curve(y_test, knn_pipeline.predict_proba(X_test)[:,1])\n",
    "\n",
    "# Calcul de l'aire sous la courbe ROC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Affichage de la courbe ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taux de Faux Positif')\n",
    "plt.ylabel('Taux de Vrai Positif')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b550e899-7887-4b1d-a58c-821fbb048edf",
   "metadata": {},
   "source": [
    " La courbe ROC évalue la capacité d'un modèle à discriminer entre les classes positives et négatives à différents seuils de classification. L'AUC varie entre 0 et 1, où une valeur de 0.5 suggère une performance équivalente à un classement aléatoire, et une valeur de 1 indique une capacité parfaite à distinguer entre les classes positives et négatives sans aucune erreur.\n",
    "\n",
    "Avec une AUC de 0.81, cela signifie que le modèle a une très bonne capacité à différencier entre les employés qui vont quitter l'entreprise et ceux qui vont rester. Plus précisément, cette valeur indique que si nous choisissons au hasard un employé ayant quitté l'entreprise (positif) et un employé ayant choisi de rester (négatif), il y a 88% de chances que le modèle classe correctement l'employé partant avec une probabilité plus élevée de quitter l'entreprise que l'employé restant.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Avec une précision globale de 86.3%, le modèle est généralement précis dans ses prédictions. Cependant, la matrice de confusion révèle certaines faiblesses, notamment avec un nombre relativement élevé de faux négatifs, où le modèle n'a pas réussi à identifier les employés qui ont effectivement quitté l'entreprise. Bien que le modèle ait correctement identifié un grand nombre de vrais négatifs, indiquant une bonne capacité à reconnaître les employés qui restent, la capacité à détecter correctement les départs est moins impressionnante. Cela suggère que le modèle pourrait être trop prudent dans la prédiction de l'attrition, ce qui pourrait être dû à un déséquilibre des classes dans les données d'entraînement où les cas de non-attrition sont beaucoup plus nombreux que les cas d'attrition.\r\n",
    "\r\n",
    "La précision de 62% pour la classe d'attrition (employés quittant l'entreprise) et le rappel de 35% pour cette même classe reflètent ces limitations. Le modèle tend à manquer un certain nombre de vrais cas d'attrition, ce qui est problématique si l'objectif est d'intervenir ou de retenir ces employés. En revanche, une AUC de 0188 indique que le modèle est assez bon pour classer un employé au hasard comme ayant plus de chances de quitter lorsque c'est effectivement le cas, comparé à un employé au hasard qui reste.\r\n",
    "\r\n",
    "Dans l'ensemble, l'utilisation de KNN pour notre projet montre un potentiel prometteur mais avec des axes d'amélioration. Il peut être nécessaire d'ajuster le nombre de voisins k, d'expérimenter avec différentes métriques de distance, ou d'employer des techniques de rééquilibrage de classe pour améliorer la sensibilité du modèle aux cas d'attrition. Il serait également judicieux de comparer KNN à d'autres modèles de classification pour s'assurer que nous utilisons le meilleur outil possible pour nos besoins spécifiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568fb5a7-bfbc-489a-b417-68e0728e4650",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Random Forest est un algorithme d'apprentissage supervisé polyvalent et robuste utilisé pour la classification et la régression. Il fonctionne en construisant un grand nombre d'arbres de décision lors de la phase d'entraînement et en produisant la classe qui est le mode des classes (classification) ou la moyenne des prédictions (régression) des arbres individuels. Random Forest est un exemple d'ensemble learning, où la combinaison des résultats de multiples modèles vise à produire une prédiction finale plus précise et plus stable.\r\n",
    "\r\n",
    "Un des principaux avantages de Random Forest est sa capacité à gérer un grand nombre de caractéristiques d'entrée et à évaluer l'importance de chaque caractéristique dans la prédiction. Cela en fait un choix excellent pour les situations où la compréhension des facteurs influents est aussi importante que la prédiction elle-même. De plus, Random Forest a tendance à éviter le surajustement aux données d'entraînement grâce à la diversité des arbres et à l'utilisation de sous-ensembles aléatoires de caractéristiques pour diviser les noeuds des arbres, ce qui le rend généralement performant sur des ensembles de données non vus.\r\n",
    "\r\n",
    "Avec sa facilité d'utilisation et sa nature peu exigeante en termes de réglage des hyperparamètres, Random Forest est souvent un bon point de départ pour les tâches de modélisation prédictive et peut servir de benchmark pour comparer les performances avec d'autres algorithmes de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62cc50-fef1-4fa0-b37e-80ea6ec28716",
   "metadata": {},
   "source": [
    "### Copie des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817d038-2196-4116-babd-f9c18f3f52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetforest = combined_csv.copy()\n",
    "dataSetforest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3224365-31a1-4c87-9ecb-3e0b2804a2a3",
   "metadata": {},
   "source": [
    "### Entraienement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d1a0d-e5e4-48e4-880f-65f1cffee732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un pipeline intégrant le préprocesseur et le modèle KNN\n",
    "random_forest_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8581d9f-2def-4a5d-a6e1-6183d6afb561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entraînement du pipeline\n",
    "random_forest_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17faeefd-ab49-4beb-b472-dd073ec5d611",
   "metadata": {},
   "source": [
    "### Prédicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c399e07-e0f9-4be3-b1b3-ca5309b6966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions sur l'ensemble de test\n",
    "y_pred = random_forest_pipeline.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d69d906-9b8d-4570-947b-019a57dfe5ec",
   "metadata": {},
   "source": [
    "Les résultats obtenus avec l'algorithme Random Forest pour notre projet de prédiction de l'attrition des employés sont très positifs.\n",
    "\n",
    "Avec une précision globale de 93.8%, le modèle Random Forest a réussi à prédire correctement si un employé allait quitter ou non l'entreprise dans la grande majorité des cas. Cette haute précision indique une adéquation forte entre les prédictions du modèle et les valeurs réelles.\n",
    "\n",
    "Le rapport de classification fournit un aperçu plus détaillé par classe :\n",
    "\n",
    "Pour la classe 0 (employés qui restent) :\n",
    "\n",
    "- Précision : 93%, ce qui signifie que presque tous les employés prédits comme restants étaient réellement des cas de non-attrition.\n",
    "- Rappel : 100%, indiquant que le modèle a identifié tous les employés qui sont effectivement restés.\n",
    "- Score F1 : 96%, une moyenne harmonique entre la précision et le rappel, suggérant un équilibre très favorable entre la précision et la sensibilité pour cette classe.\n",
    "\n",
    "Pour la classe 1 (employés qui quittent) :\n",
    "- Précision : 100%, montrant que tous les employés prédits comme partants ont effectivement quitté l'entreprise.\n",
    "- Rappel : 61%, ce qui signifie que le modèle a pu identifier une grande partie, mais pas la totalité, des cas réels d'attrition.\n",
    "- Score F1 : 75%, reflétant une performance solide mais légèrement inférieure à celle de la classe 0, due à un rappel plus faible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ddc523-a762-450d-a34b-69ae795bc42e",
   "metadata": {},
   "source": [
    "### Matrice de confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb752f0-daa2-4545-8cd7-1911f7ab6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)  # Utilisez y_pred_best si vous avez fait de l'optimisation hyperparamètre\n",
    "# Ou utilisez y_pred pour le modèle sans optimisation\n",
    "\n",
    "# Visualisation de la matrice de confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non', 'Oui'], yticklabels=['Non', 'Oui'])\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Valeurs Réelles')\n",
    "plt.title('Matrice de Confusion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9fd17-81e1-4546-9122-389c4326e7d5",
   "metadata": {},
   "source": [
    "La matrice de confusion pour le modèle Random Forest présente les performances de prédiction de l'attrition des employés avec une grande clarté. Voici le détail :\n",
    "\n",
    "- Vrais Négatifs (TN) : 1115 employés ont été correctement identifiés comme ne quittant pas l'entreprise, ce qui indique que le modèle est extrêmement précis pour détecter les cas de non-attrition.\n",
    "- Faux Positifs (FP) : Aucun employé n'as été incorrectement prédits comme quittant l'entreprise alors qu'en réalité ils sont restés, ce qui montre une très faible erreur de Type I.\n",
    "- Faux Négatifs (FN) : 82 employés ont été incorrectement prédits comme restant alors qu'ils ont effectivement quitté l'entreprise. Cela représente une erreur de Type II, où le modèle n'a pas détecté l'attrition.\n",
    "- Vrais Positifs (VP) : 126 employés qui ont quitté l'entreprise ont été correctement identifiés comme tels.\n",
    "\n",
    "Cette matrice révèle que le modèle Random Forest a une excellente capacité à reconnaître les employés qui resteront dans l'entreprise, avec un taux très élevé de vrais négatifs et un nombre très faible de faux positifs. La performance est également très bonne pour identifier les employés qui partent, avec un nombre élevé de vrais positifs par rapport aux faux négatifs, bien que cet aspect puisse encore être amélioré."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf04292-cc29-4fe1-a945-d1e68af0a347",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a0efc2-78f0-45a6-883e-1a71436f8812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des taux de vrais positifs et faux positifs\n",
    "fpr, tpr, thresholds = roc_curve(y_test, random_forest_pipeline.predict_proba(X_test)[:,1])\n",
    "\n",
    "# Calcul de l'aire sous la courbe ROC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Affichage de la courbe ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taux de Faux Positif')\n",
    "plt.ylabel('Taux de Vrai Positif')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6361bfe-eefa-46d1-8ee3-b574de0d7bb9",
   "metadata": {},
   "source": [
    "La courbe ROC affichée illustre de manière visuelle l'excellente performance du modèle Random Forest dans notre contexte de prédiction de l'attrition. La ligne orange représente la courbe ROC du modèle, tandis que la ligne pointillée bleue représente la performance d'un modèle aléatoire.\r\n",
    "\r\n",
    "Courbe ROC : La courbe de notre modèle suit de très près le coin supérieur gauche, indiquant un taux élevé de vrais positifs et un faible taux de faux positifs à travers différents seuils de décision. Cela signifie que le modèle est capable de distinguer avec précision entre les employés qui quitteront et ceux qui resteront dans l'entreprise.\r\n",
    "\r\n",
    "AUC de80.99 : L'aire sous la courbe (AUC) est proche de la perfection. Une valeur de80.99 sur une échelle de 0 à 1 indique que le modèle a une probabilité 8e 99% de correctement classer un employé positif (qui quitte) plus haut qu'un employé négatif (qui reste), ce qui est confirmé par la proximité de la courbe ROC avec le coin supérieur gauche et le bord supérieur du graphique.\r\n",
    "\r\n",
    "Cette courbe ROC, couplée à une AUC de 0.99, suggère que le modèle est très performant et qu'il offre une confiance élevée dans ses prédictions. Dans la pratique, cela signifie que les stratégies de rétention des employés fondées sur les prédictions de ce modèle ont une grande chance de cibler correctement les individus à risque. Il est rare d'obtenir de tels résultats, ce qui peut aussi inciter à une vérification approfondie pour s'assurer qu'il n'y a pas de surajustement ou de biais dans le modèle ou les données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec2454-ae98-41f7-a5d8-3428d52ea2ab",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "La performance globale du modèle Random Forest, illustrée par la courbe ROC, la matrice de confusion, et les métriques de précision, est nettement supérieure à celle que nous avons observée avec le modèle KNN.\r\n",
    "\r\n",
    "Le Random Forest a démontré unbonne e capacile à distinguer entre les employés qui vont quitter l'entreprise et ceux qui vont rester, comme en témoigne son AUC presque parfaite. Les erreurs de classification sont minimales, et les mesures de précision et de rappel indiquent que le modèle est à la fois précis et sensible dans ses prédictions.\r\n",
    "\r\n",
    "En comparaison, bien que le KNN ait fourni des résultats satisfaisants, le Random Forest se distingue par une performance supérieure et une fiabilité accrue. La différence entre les deux modèles pourrait être attribuée à la capacité du Random Forest à gérer les caractéristiques complexes et les interactions non linéaires entre elles, ce qui est souvent le cas dans les données réelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c3be7e-b5c1-47f7-8b30-4f6106afbae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba3aa79-7c1f-49c4-ae52-5aa41cdae0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
